# -*- coding: utf-8 -*-
"""Web-Scrapping and Langchain

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xrsfyp9rVAiCb63JIfEV682isk-37iCL
"""

!pip install beautifulsoup4
!pip install cohere
!pip install langchain cohere faiss-cpu
!pip install openai
!pip install -U langchain-community
!pip install langchain faiss-cpu cohere tiktoken
!pip install flask flask-ngrok

with open("requirements.txt", "w") as f:
    f.write("""bs4
cohere
langchain
pydantic
requests
""")

from google.colab import files
files.download("requirements.txt")

import requests
from bs4 import BeautifulSoup

web=requests.get("https://www.ekaimpact.org/")
print(web)

web.content

soup=BeautifulSoup(web.content,"html.parser")

print(soup.prettify())

tag=soup.html

type(tag)

tag=soup.a
tag

soup.title

soup.head()

soup.body

soup.find("h1")

soup.find_all("h1")

class_data=soup.find("div",class_="wixDesktopViewport")

for i in soup.find_all("a"):
  print(i.get("href"))

img=soup.find_all("img")

for i in img:
  print(i.get("src"))

text=soup.get_text(separator="/n",strip=True)

with open("eka_site_content.txt", "w", encoding="utf-8") as f:
    f.write(text)

print("‚úÖ Scraped content saved to eka_site_content.txt")

with open("eka_site_content.txt","r",encoding="utf-8")as f:
  raw_text=f.read()

  lines=[line.strip()for line in raw_text.split("\n")if len(line.strip())>40]
  cleaned_text="\n".join(lines)
  with open("eka_cleaned.txt", "w", encoding="utf-8") as f:
    f.write(cleaned_text)

import cohere


co = cohere.Client("zJNR0dOqvUypmGWWcCZJdyaH5WCa1uyViiF3qHv8")

with open("eka_cleaned.txt", "r", encoding="utf-8") as f:
    content = f.read()

prompt = f"""Summarize the NGO website content below:

{content}

Summary:
"""

response = co.generate(
    model="command",
    prompt=prompt,
    max_tokens=400,
    temperature=0.5
)

summary = response.generations[0].text.strip()


with open("summary.txt", "w", encoding="utf-8") as f:
    f.write(summary)

print("‚úÖ Summary saved to summary.txt")

from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
loader=TextLoader("eka_site_content.txt")
documents=loader.load()

text_splitter=CharacterTextSplitter(
    separator="\n",
    chunk_size=300,
    chunk_overlap=50,
)

docs=text_splitter.split_documents(documents)

import cohere
from langchain.embeddings.base import Embeddings

class MyCohereEmbedder(Embeddings):
    def __init__(self, api_key: str):
        self.client = cohere.Client(api_key)

    def embed_documents(self, texts):
        response = self.client.embed(
            texts=texts,
            model="embed-english-v3.0",
            input_type="search_document"
        )
        return response.embeddings

    def embed_query(self, text):
        response = self.client.embed(
            texts=[text],
            model="embed-english-v3.0",
            input_type="search_query"
        )
        return response.embeddings[0]

from langchain.llms.base import LLM
from pydantic import Field
from typing import Optional, List
import cohere

class CohereLLM(LLM):
    api_key: str
    model: str = "command"
    temperature: float = 0.5
    max_tokens: int = 1000
    cohere_client: Optional[cohere.Client] = Field(default=None, exclude=True)

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.cohere_client = cohere.Client(self.api_key)

    @property
    def _llm_type(self) -> str:
        return "cohere-custom"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        response = self.cohere_client.generate(
            model=self.model,
            prompt=prompt,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            stop_sequences=stop or []
        )
        return response.generations[0].text.strip()

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

llm = CohereLLM(api_key="zJNR0dOqvUypmGWWcCZJdyaH5WCa1uyViiF3qHv8")

from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),

)


result = qa.run("What kind of social work does EkaImpact do?")
print(result)

result = qa.run("Where is EkaImpact based?")
print(result)

result = qa.run("Who founded EkaImpact?")
print(result)

result=qa.run("Who all are in the team of ekaimpact?")
print(result)

result = qa.run("What are the main programs run by EkaImpact?")
print(result)

result = qa.run("What does EkaImpact do in the healthcare sector?")
print(result)

result = qa.run("what is Neev Ki Eent Foundation")
print(result)

result = qa.run("What is EkaImpact Vison?")
print(result)

result = qa.run("What is EkaImpact mission?")
print(result)

result=qa.run("How can we join Ekaimpact?")
print(result)

result=qa.run("how can we get in touch with Ekaimpact?")
print(result)

result=qa.run("who all are in the team of ekaimapct?")
print(result)

from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import CohereEmbeddings
from langchain.vectorstores import FAISS
from langchain.docstore.document import Document

# 1. Load your text file
with open("eka_cleaned.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()

# 2. Split into chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
texts = text_splitter.split_text(raw_text)

# 3. Create Document objects
docs = [Document(page_content=t) for t in texts]

# 4. Create embeddings
embedding = CohereEmbeddings(cohere_api_key="zJNR0dOqvUypmGWWcCZJdyaH5WCa1uyViiF3qHv8", user_agent="your-app")

# 5. Build and save FAISS index
vectorstore = FAISS.from_documents(docs, embedding)
vectorstore.save_local("faiss_index")

print(f"‚úÖ Total chunks: {len(texts)}")

from flask import Flask, request, jsonify
from langchain.vectorstores import FAISS
from langchain_community.llms import Cohere
from langchain.chains import RetrievalQA
from langchain_community.embeddings import CohereEmbeddings

app = Flask(__name__)

# Load embeddings and vector store
embedding = CohereEmbeddings(cohere_api_key="zJNR0dOqvUypmGWWcCZJdyaH5WCa1uyViiF3qHv8", user_agent="your-app")
vectorstore = FAISS.load_local("faiss_index", embedding, allow_dangerous_deserialization=True)

llm = Cohere(cohere_api_key="zJNR0dOqvUypmGWWcCZJdyaH5WCa1uyViiF3qHv8", model="command", max_tokens=500)
qa = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever(), return_source_documents=False)

@app.route("/chat", methods=["POST"])
def chat():
    query = request.json.get("query")
    result = qa.run(query)
    return jsonify({"response": result})

if __name__ == "__main__":
    app.run()

from langchain_community.embeddings import CohereEmbeddings
from langchain_community.vectorstores import FAISS

embedding = CohereEmbeddings(cohere_api_key="zJNR0dOqvUypmGWWcCZJdyaH5WCa1uyViiF3qHv8", user_agent="your-app")
print("üîÅ Generating embedding...")
vectorstore = FAISS.from_documents(docs, embedding)
print("‚úÖ Embedding complete")

print("üíæ Saving FAISS index...")
vectorstore.save_local("faiss_index")
print("‚úÖ All done")

!pip install -r requirements.txt

vectorstore.save_local("faiss_index")