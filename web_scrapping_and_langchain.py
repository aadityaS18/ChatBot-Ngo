# -*- coding: utf-8 -*-
"""Web-Scrapping and Langchain

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xrsfyp9rVAiCb63JIfEV682isk-37iCL
"""

!pip install beautifulsoup4
!pip install cohere
!pip install langchain cohere faiss-cpu
!pip install openai
!pip install -U langchain-community
!pip install langchain faiss-cpu cohere tiktoken
!pip install flask flask-ngrok

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time


base_url = "https://www.ekaimpact.org"
visited = set()
all_text = []

def is_valid_url(url):
    """Checks if a URL is internal and not already visited."""
    parsed = urlparse(url)
    return (parsed.netloc == "" or parsed.netloc == "www.ekaimpact.org") and url not in visited

def extract_text_from_html(html):
    """Extract visible text from HTML using BeautifulSoup."""
    soup = BeautifulSoup(html, "html.parser")

    # Remove script and style elements
    for script_or_style in soup(["script", "style"]):
        script_or_style.extract()

    # Get text and clean
    text = soup.get_text(separator="\n", strip=True)
    return text

def scrape_all_pages(url):
    """Recursively scrapes internal pages and extracts text."""
    if not is_valid_url(url):
        return

    visited.add(url)
    print(f"ðŸ” Scraping: {url}")

    try:
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            print(f"âŒ Failed: {url}")
            return

        html = response.text
        text = extract_text_from_html(html)
        all_text.append(text)

        # Parse page and find internal links
        soup = BeautifulSoup(html, "html.parser")
        for a_tag in soup.find_all("a", href=True):
            link = urljoin(url, a_tag["href"])
            if is_valid_url(link):
                scrape_all_pages(link)
                time.sleep(1)  # polite delay

    except Exception as e:
        print(f"âš ï¸ Error scraping {url}: {e}")

# Start scraping
scrape_all_pages(base_url)

# Save result
with open("ekaimpact_full_site.txt", "w", encoding="utf-8") as f:
    f.write("\n\n".join(all_text))

print(f"\nâœ… Done! Scraped {len(visited)} pages.")

with open("requirements.txt", "w") as f:
    f.write("""bs4
cohere
langchain
pydantic
requests
""")

from google.colab import files
files.download("requirements.txt")

with open("ekaimpact_full_site.txt","r",encoding="utf-8")as f:
  raw_text=f.read()

lines=[line.strip()for line in raw_text.split("\n") if len(line.strip())>40 ]
cleaned_text="\n".join(lines)

with open("ekaimpact_cleaned.txt","w",encoding="utf-8")as f:
  f.write(cleaned_text)

## ekaimpact cleaned text file is file without unwanted short lines and unecessary whitespace

print("Cleaned text saved to ekaimpact_cleaned.txt")

import cohere


co = cohere.Client("zJNR0dOqvUypmGWWcCZJdyaH5WCa1uyViiF3qHv8")

with open("ekaimpact_cleaned.txt", "r", encoding="utf-8") as f:
    context = f.read()
    context=context[:12000]

system_prompt = (
    "Use the context below to summarize in 3-4 lines. "
    "Be concise and avoid repetition.\n\n"
    "Context:\n{context}\n\nSummary:"
)


response = co.generate(
    model="command",
    prompt=system_prompt.format(context=context),
    max_tokens=400,
    temperature=0.5
)

summary = response.generations[0].text.strip()


with open("summary.txt", "w", encoding="utf-8") as f:
    f.write(summary)

print("âœ… Summary saved to summary.txt")

from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.text_splitter import CharacterTextSplitter

# Load your cleaned text
with open("ekaimpact_full_site.txt", "r", encoding="utf-8") as f:
    content = f.read()

# Split content into smaller chunks
splitter = CharacterTextSplitter(separator="\n", chunk_size=2000, chunk_overlap=100)
chunks = splitter.split_text(content)

# Save chunks (optional)
with open("ekaimpact_chunks.txt", "w", encoding="utf-8") as f:
    for chunk in chunks:
        f.write(chunk + "\n---\n")

import cohere
from langchain.embeddings.base import Embeddings

class MyCohereEmbedder(Embeddings):
    def __init__(self, api_key: str):
        self.client = cohere.Client(api_key)

    def embed_documents(self, texts):
        response = self.client.embed(
            texts=texts,
            model="embed-english-v3.0",
            input_type="search_document"
        )
        return response.embeddings

    def embed_query(self, text):
        response = self.client.embed(
            texts=[text],
            model="embed-english-v3.0",
            input_type="search_query"
        )
        return response.embeddings[0]

from langchain.llms.base import LLM
from pydantic import Field
from typing import Optional, List
import cohere

class CohereLLM(LLM):
    api_key: str
    model: str = "command"
    temperature: float = 0.5
    max_tokens: int = 1000
    cohere_client: Optional[cohere.Client] = Field(default=None, exclude=True)

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.cohere_client = cohere.Client(self.api_key)

    @property
    def _llm_type(self) -> str:
        return "cohere-custom"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        response = self.cohere_client.generate(
            model=self.model,
            prompt=prompt,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            stop_sequences=stop or []
        )
        return response.generations[0].text.strip()

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import CohereEmbeddings
from langchain.schema import Document

# Step 1: Load cleaned text
with open("ekaimpact_cleaned.txt", "r", encoding="utf-8") as f:
    ekaimpact_text = f.read()

# Step 2: Chunk it
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_text(ekaimpact_text)

# Step 3: Convert to Document objects
docs = [Document(page_content=chunk) for chunk in chunks]

# Step 4: Create embeddings
embedding = CohereEmbeddings(
    cohere_api_key="zJNR0dOqvUypmGWWcCZJdyaH5WCa1uyViiF3qHv8",
    user_agent="your-app"
)

# Step 5: Store in FAISS
vectorstore = FAISS.from_documents(docs, embedding)
vectorstore.save_local("faiss_index")

print("âœ… FAISS index built and saved.")

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

llm = CohereLLM(api_key="zJNR0dOqvUypmGWWcCZJdyaH5WCa1uyViiF3qHv8")

from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),

)


result = qa.run("What kind of social work does EkaImpact do?")
print(result)

result = qa.run("Who founded EkaImpact?")
print(result)

result=qa.run("Who all are in the team of ekaimpact?")
print(result)

result = qa.run("What are the main programs run by EkaImpact?")
print(result)

result = qa.run("What does EkaImpact do in the healthcare sector?")
print(result)

result=qa.run("who all are in the team of ekaimapct?")
print(result)

from flask import Flask, request, jsonify
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_community.embeddings import CohereEmbeddings
from langchain_community.llms import Cohere

app = Flask(__name__)

embedding = CohereEmbeddings(cohere_api_key="zJNR0dOqvUypmGWWcCZJdyaH5WCa1uyViiF3qHv8",user_agent="ekaimpact-chatbot/1.0")
vectorstore = FAISS.load_local("faiss_index", embedding, allow_dangerous_deserialization=True)
llm = Cohere(cohere_api_key="zJNR0dOqvUypmGWWcCZJdyaH5WCa1uyViiF3qHv8", model="command", max_tokens=500)
qa = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())

@app.route("/", methods=["GET"])
def home():
    return "EkaImpact Chatbot Backend is running!"

@app.route("/chat", methods=["POST"])
def chat():
    query = request.json.get("query")
    result = qa.invoke(query)  # <--- modern usage
    return jsonify({"response": result})